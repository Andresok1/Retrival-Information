{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 7: Text classification Naive Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes inferencing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider you are building a system that can decide the author given the snippet of a book. Assume you simplify the problem so that the \"documents\" are represented by their key words (in other words, stop words are removed plus other preprocessing steps). You have annotated some of the training data as follows:\n",
    "\n",
    "| doc_id | Document | label |\n",
    "| --- | --- | --- |\n",
    "| 1 | dialectic dasein dialectic | Hegel |\n",
    "| 2 | dialectic analysis ideology | Hegel |\n",
    "| 3 | utopia justice analysis | Plato |\n",
    "| 4 | Lacanian analysis ideology | Zizek |\n",
    "| 5 | justice Marx dialectic ideology | Zizek |\n",
    "\n",
    "Now you are faced with the following sentence:\n",
    "\n",
    "q = ```a dialectic analysis of ideology```\n",
    "\n",
    "What is the expected prediction of your system? Please ignore out-of-vocabulary terms while prediction, and use Add-1 (Laplace) smoothing for computing the likelihood probabilities."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer goes here:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Macro/Micro measures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider one of your models uses a 1-vs-rest strategy* to do the multi-class classification. The results shows as follows:\n",
    "\n",
    "class name = Hegel\n",
    "\n",
    "| | in the class | not in the class |\n",
    "| --- | --- | --- |\n",
    "| predict to be in the class | 20 | 10 |\n",
    "| predicted to not be in the class |  10 | 60 |\n",
    "\n",
    "class name = Plato\n",
    "\n",
    "|| in the class | not in the class |\n",
    "| --- | --- | --- |\n",
    "| predicted to be in the class | 15 | 5 |\n",
    "| predicted to not be in the class | 15 | 65 |\n",
    "\n",
    "class name = Zizek\n",
    "\n",
    "|| in the class | not in the class |\n",
    "| --- | --- | --- |\n",
    "| predicted to be in the class | 40 | 10 |\n",
    "| predicted to not be in the class | 0 | 50 |\n",
    "\n",
    "1. Calculate the counts of instances in these three classes respectively.\n",
    "2. Calculate the precision/recall for the three classes\n",
    "3. Calculate the micro and macro F1 of this model\n",
    "\n",
    "*1-vs-all strategy means a multi-class classification strategy, where we transfer the multi-class classification into a binary classification. Its two classes are \"to be this class\" and \"to be the rest classes\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer goes here:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector space Classification\n",
    "\n",
    "1. What are two premises of vector space classification?\n",
    "2. Why do we need those?\n",
    "3. Do they hold in practice?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer goes here:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
