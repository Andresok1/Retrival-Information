{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5: Vector Space Model, Scoring"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring: query-document similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: The Mechanics of tf-idf\n",
    "\n",
    "Part A: Definitions and Formulas\n",
    "\n",
    "a) Define $tf_{t,d}$: What does this represent in the context of a document and a term? \n",
    "\n",
    "b) Define $df_t$: How is this different from $tf$? Why do we prefer using $df$ over collection frequency ($cf$) for discrimination? \n",
    "\n",
    "c) Define $idf_t$: Write the formula for Inverse Document Frequency. Explain what happens to the $idf$ value as a term becomes more rare. \n",
    "\n",
    "d) Define $tf\\text{-}idf_{t,d}$: Write the full formula combining the parts above. \n",
    "\n",
    "\n",
    "Part B: Calculating Inverse Document Frequency (idf)\n",
    "\n",
    "Consider a collection size $N = 1,000,000$. Calculate the $idf$ for the following two terms using base-10 logarithms ($\\log_{10}$).\n",
    "\n",
    "Term A: Appears in $1,000$ documents ($df_A = 1,000$).\n",
    "\n",
    "Term B: Appears in $500,000$ documents ($df_B = 500,000$).\n",
    "\n",
    "\n",
    "\n",
    "Part C: Calculating Final Weights\n",
    "\n",
    "Now, consider a specific Document $d_1$.\n",
    "\n",
    "Term A appears $5$ times in $d_1$ ($tf_{A,d1} = 5$).\n",
    "\n",
    "Term B appears $20$ times in $d_1$ ($tf_{B,d1} = 20$).\n",
    "\n",
    "Using your results from Part B, calculate the final $tf\\text{-}idf$ score for both Term A and Term B in Document $d_1$.\n",
    "\n",
    "\n",
    "Part D: Analysis\n",
    "\n",
    "Compare the two scores from Part C.\n",
    "\n",
    "Which term has the higher weight? \n",
    "\n",
    "Explain why Term B, despite appearing 4 times more often than Term A in this specific document, has a lower weight. Relate this to the concept of discriminating power. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Cosine Similarity vs. Euclidean Distance\n",
    "\n",
    "Part A: The Problem with Euclidean Distance\n",
    "\n",
    "In the vector space model, we represent documents as vectors of term weights. Explain intuitively why using Euclidean distance (the straight-line distance between two points) might be a poor metric for determining if two documents are similar in topic, especially if one document is very long and the other is very short.\n",
    "\n",
    "How does Cosine Similarity solve this specific problem? (Hint: Think about what the angle represents versus the magnitude).\n",
    "\n",
    "Part B: \n",
    "Let $\\vec{q}$ be a query vector and $\\vec{d}$ be a document vector. Write the formula for the Euclidean distance ($|\\vec{q} - \\vec{d}|$) between these two vectors Write the formula for the Cosine similarity between these two vectors. What happens to the magnitude (length) of these vectors if we normalize them to be unit vectors?\n",
    "\n",
    "Part C:  Assume that the query vector $\\vec{q}$ and the document vector $\\vec{d}$ have both been normalized to unit length (i.e., $|\\vec{q}| = 1$ and $|\\vec{d}| = 1$). Expand the square of the Euclidean distance formula: $|\\vec{q} - \\vec{d}|^2 = (\\vec{q} - \\vec{d}) \\cdot (\\vec{q} - \\vec{d})$. Simplify this expression using the dot product properties.Substitute the known unit lengths. Show how the result relates directly to the Cosine similarity formula.\n",
    "\n",
    "Part D: Based on your derivation in Part C: If we rank documents by increasing Euclidean distance (smallest distance first), will we get a different order than if we rank them by decreasing Cosine similarity (largest similarity first)? Explain your reasoning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 Vector Space Scoring (lnc.ltc weighting variant)\n",
    "\n",
    "As described in the textbook, lnc.ltc is a common weighting strategy where one puts the $idf$ weight on the query vector only, while the document vector uses only term frequency. This avoids the \"Double Counting\" of IDF when performing (normalized) dot product.\n",
    "\n",
    "Calculate the Cosine Similarity score between a query vector and a document vector using the following data:\n",
    "\n",
    "Query: \"digital cameras\"\n",
    "\n",
    "Document: \"digital cameras and video cameras\"\n",
    "\n",
    "N (Total Docs): 10,000\n",
    "\n",
    "Assumptions: The term \"and\" is a stop word (weight = 0).\n",
    "\n",
    "$df_{\\text{digital}} = 100$\n",
    "\n",
    "$df_{\\text{video}} = 1,000$\n",
    "\n",
    "$df_{\\text{cameras}} = 500$\n",
    "\n",
    "Use logarithmic weighting for the query vector: $w_{t,q} = (1 + \\log(tf_{t,q})) \\times idf_t$. (Note: Use log with base 10)\n",
    "\n",
    "\n",
    "Use only raw $tf$ for the document vector (no $idf$, no log).\n",
    "\n",
    "Normalize both vectors before taking the dot product."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
