{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Index Construction - Solved"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. a) In your own words, describe the step-by-step process the Blocked Sort-Based Indexing (BSBI) algorithm uses to build an inverted index.\n",
    "   b) In your own words, describe the step-by-step process the Single-Pass In-Memory Indexing (SPIMI) algorithm uses to build an inverted index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BSBI algorithm is an external sorting algorithm designed to build an index for collections that are too large to fit in memory. It works by breaking the problem into \"blocks\" that can fit in memory.The process is as follows:\n",
    "\n",
    "\n",
    "i) Parse and Batch: The algorithm parses the document collection and accumulates (termID, docID) pairs in memory until it fills a block of a fixed, predefined size. Term to termID mapping is done using a dictionary and is created on the fly or can be pre-computed with a first pass through the block of documents.\n",
    "\n",
    "\n",
    "ii) In-Memory Sort: It performs a fast in-memory sort on this single block of pairs, sorting them by termID (primary key) and docID (secondary key).\n",
    "\n",
    "\n",
    "iii) Write to Disk: This sorted block (which is now a small, inverted index for one part of the collection) is written to a temporary file on disk.\n",
    "\n",
    "\n",
    "iv) Repeat: The algorithm repeats steps 1-3, creating multiple sorted block files on disk (e.g., $f_1, f_2, \\dots, f_n$) until the entire collection has been processed.Merge: In a final step, the algorithm opens all the temporary block files on disk and simultaneously merges them into one large, final inverted index. This merge is efficient because it only requires sequential reads from the sorted blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SPIMI algorithm also processes the collection in blocks, but it is more efficient and scalable.\n",
    "\n",
    "The process is as follows:\n",
    "\n",
    "i) Parse and create postings on the fly: SPIMI reads tokens (term, docID) one by one. It uses a hash table in memory as its dictionary. For each token, it looks up the term in the hash table and adds the docID directly to that term's postings list.\n",
    "\n",
    "ii) Write to Disk: It continues this until it runs out of memory. At this point, it sorts the terms in its dictionary (a small sort) and writes the entire block (dictionary and postings lists) to a temporary file on disk.\n",
    "\n",
    "iii) Repeat and Merge: It then frees its memory, creates a new empty dictionary, and starts processing the next block. Like BSBI, it finishes with a final merge of all sorted blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. State one advantage and one disadvantage of Blocked sort-based Indexing (BSBI) over Single-pass in-memory indexing (SPIMI). State one similarity between them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:\n",
    "\n",
    "**Advantage of BSBI over SPIMI**: BSBI does not have much advantage over SPIMI though one minor advantage is that its memory management is simpler. It converts all terms to fixed-size termIDs (e.g., 4-byte integers). This makes the data being sorted (the (termID, docID) pairs) uniform and predictable in size. SPIMI, by contrast, must handle dynamically growing postings lists in memory\n",
    "\n",
    "**Disadvantage of BSBI over SPIMI**: The two major disadvantages of BSBI are:Scalability: BSBI requires the entire term-to-termID dictionary (the global vocabulary) to fit in main memory. If the vocabulary is too large, the algorithm fails.Time Complexity: BSBI must perform an expensive sorting step on all (termID, docID) pairs in the collection. This gives it a computational complexity of $\\Theta(T \\log T)$. SPIMI avoids this massive sort, giving it a more efficient linear time complexity of $\\Theta(T)$.\n",
    "\n",
    "**Similarity:** Both BSBI and SPIMI process the collection by (1) segmenting it into blocks that can fit in memory, (2) creating an inverted index for each block, and (3) writing these intermediate inverted blocks to disk. Both algorithms conclude with a final merge step that combines all the sorted blocks into one large, final index."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. State two issues faced with main and auxiliary indexes? In the context of websites, state one advantage of dynamic indexing?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: \n",
    "\n",
    "Two issues with main and auxiliary indexes:\n",
    "\n",
    "1. Costly Merges: Each time the small auxiliary index is merged into the large main index, it's an expensive operation. If the index is stored as one large file, this merge requires re-writing the entire main index, leading to an inefficient $\\Theta(T^2/n)$ overall complexity.\n",
    "\n",
    "2. Slower Query Performance:\n",
    "\n",
    "During Search: Queries must be run across both the main index and the auxiliary index, and the results must be merged, which is slower than querying a single index.\n",
    "\n",
    "During Merge: The merging process itself is resource-intensive and can temporarily slow down the entire search system.\n",
    "\n",
    "\n",
    "**Advantage of dynamic indexing:** The key advantage is immediacy: Dynamic indexing allows new documents (like a new blog post, news story, or forum comment on a website) to be added to the index and become searchable almost immediately. This is a huge improvement over \"periodic reconstruction,\" where the new content might not be searchable until the entire index is rebuilt from scratch, which could be hours or even a day later."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Numerical:**  If we need $n log_2$ n comparisons (where $n$ is the number of termID-docID pairs) and $2$ disk seeks for each comparison, how much time would index construction for Reuters-RCV1 take if we used disk instead of memory for storage and an unoptimized sorting algorithm (i.e., not an external sorting algorithm)? Please show the steps and provide necessary explanation.\n",
    "\n",
    "Assume, disk seek time  $= 5*10^{-3}$ seconds and $n$ for the Reuters-RCV1 corpus $= 100$ million $= 10^8$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:\n",
    "\n",
    "Our goal is to find the Total Time to build the index. The total time is the product of how many comparisons we must do and how much time each comparison costs.\n",
    "\n",
    "$\\text{Total Time} = (\\text{Total Comparisons}) \\times (\\text{Time per Comparison})$\n",
    "\n",
    "\n",
    "The problem states that an unoptimized sort requires $n \\log_2 n$ comparisons, where $n$ is the number of `termID-docID` pairs.\n",
    "\n",
    "* $n$: $100 \\text{ million} = 10^8$\n",
    "* $\\log_2 n$: $\\log_2(10^8)$\n",
    "   * Using the logarithm power rule ($\\log(x^y) = y \\cdot \\log(x)$), this becomes: $8 \\times \\log_2(10)$\n",
    "   * Since $\\log_2(10)$ is approximately $3.32$, this is: $8 \\times 3.3219 \\approx 26.575$\n",
    "* Total Comparisons: $n \\times \\log_2 n = 10^8 \\times 26.575$\n",
    "\n",
    "A \"naive\" algorithm would try to compare items stored on disk, forcing the disk head to move for each comparison.\n",
    "\n",
    "* The problem states each comparison requires 2 disk seeks (one seek to read the first item, one seek to read the second item).\n",
    "* The time for one disk seek is $5 \\times 10^{-3}$ seconds.\n",
    "* Time per Comparison: $2 \\text{ seeks} \\times (5 \\times 10^{-3} \\text{ s/seek}) = 10 \\times 10^{-3} \\text{ s} = \\mathbf{10^{-2} \\text{ s}}$\n",
    "\n",
    "* Total Time: $(\\text{Total Comparisons}) \\times (\\text{Time per Comparison})$\n",
    "* Total Time: $(10^8 \\times 26.575) \\times (10^{-2} \\text{ s})$\n",
    "* Total Time: $26.575 \\times (10^8 \\times 10^{-2}) \\text{ s}$\n",
    "* Total Time: $26.575 \\times 10^6 \\text{ seconds}$\n",
    "* Total Time: 26,575,000 seconds\n",
    "\n",
    "To put this number in context, 26,575,000 seconds is over 307 days\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_data_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
